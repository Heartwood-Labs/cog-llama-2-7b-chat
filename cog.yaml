build:
  # set to true if your model requires a GPU
  gpu: true
  cuda: "11.8"
  python_version: "3.11"
  python_packages:
    - "transformers==4.33.2"
    - "safetensors>=0.3.1"
    - "accelerate"
    # - "auto_gptq"
    - "optimum"
    - "https://r2.drysys.workers.dev/torch/11.8/torch-2.1.0-cp311-cp311-linux_x86_64.whl"
    # This wheel can be built by running `TORCH_CUDA_ARCH_LIST="8.0;8.6" pip wheel .` in https://github.com/replicate/vllm-with-loras
    - "https://r2.drysys.workers.dev/vllm/11.8/vllm-0.2a0-cp311-cp311-linux_x86_64.whl"
    - "https://r2.drysys.workers.dev/xformers/11.8/xformers-0.0.23+b4c853d.d20231107-cp311-cp311-linux_x86_64.whl"

  run:
    - pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
    - curl -o /usr/local/bin/pget -L "https://github.com/replicate/pget/releases/download/v0.1.1/pget" && chmod +x /usr/local/bin/pget
    # since we can't do LD_LIBRARY_PATH=torch/lib, use this to make sure mlc can access the cuda libs bundled with torch
    - bash -c 'ln -s /usr/local/lib/python3.11/site-packages/torch/lib/lib{nv,cu}* /usr/lib'

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"
